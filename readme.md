

# FedLoRA: Federated Learning with Low-Rank Adaptation



## ğŸ” Overview

**FedLoRA** is a flexible, production-ready codebase for **Federated Learning (FL)** research and applications. It provides plug-and-play implementations of:

- **LoRA** (Low-Rank Adaptation) for **efficient fine-tuning** of large models in federated settings.
- **Advanced FL algorithms** such as `FedAvg`, `FedLORA`, `FFA-LORA`, `AR-LORA`, `SAM-LORA`, etc.
- **Differential Privacy** support via DP-SGD.
- **Dirichlet-based data partitioning** to simulate non-IID client data distributions.
- **Multi-GPU training** with Ray for scalability.

### Key Features

| Feature | Description |
| --- | --- |
| **Models** | `BERT`, `RoBERTa`, `ViT`, `Swin`, `ResNet`, `LeNet`, etc. |
| **Datasets** | `CIFAR-10/100`, `Tiny-ImageNet`, `SST-2`, `QQP`, `MNLI`, `RTE`, `MRPC`, `QNLI`, `COLA`, `WNLI`, `STS-B` |
| **Algorithms** | `FedAvg`, `FedLORA`, `FFA-LORA`, `AR-LORA`, `SAM-LORA`, `FedAdam`, `DP-FedAvg`, `SCAFFOLD`, etc. |
| **Efficiency** | LoRA reduces communication/computation overhead by **>100Ã—** compared to full fine-tuning. |
| **Privacy** | Built-in support for **DP-SGD** with clipping and noise addition. |
| **Scalability** | Ray-based distributed training with **multi-GPU** support. |

---

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repo
git clone https://github.com/your-username/FedLoRA.git
cd FedLoRA

# Install dependencies
pip install -r requirements.txt
```

# ğŸš€ FedLoRA ä½¿ç”¨æŒ‡å—ï¼ˆCIFAR-100 ç¤ºä¾‹ï¼‰

æœ¬ä»“åº“æ”¯æŒåœ¨ **CIFAR-100** ä¸Šç”¨ **Swin-Base** æ¨¡å‹è¿è¡Œ **LA-LORA** è”é‚¦å­¦ä¹ ç®—æ³•ã€‚  
ä»¥ä¸‹æ˜¯ä½ æä¾›çš„è¿è¡Œå‘½ä»¤çš„å®Œæ•´è§£æä¸è¯´æ˜ã€‚

---

## ğŸƒ è¿è¡Œå‘½ä»¤

```bash
python main_lora.py \
  --alg LA-LORA \
  --lr 0.0001 \
  --data_name CIFAR100 \
  --alpha_value 0.1 \
  --epoch 101 \
  --extname CIFAR100 \
  --lr_decay 0.99 \
  --CNN swin_base \
  --E 1 \
  --batch_size 16 \
  --gpu 2 \
  --p 1 \
  --num_gpus_per 0.25 \
  --selection 0.5 \
  --rho 0.1 \
  --num_workers 8 \
  --pre 1 \
  --preprint 5 \
  --lora 1 \
  --r 16 \
  --freeze 0 \
  --K 50 \
  --optimizer AdamW
```

---

## ğŸ“– å‚æ•°è¯¦è§£

| å‚æ•° | å«ä¹‰ | ç¤ºä¾‹å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--alg` | è”é‚¦å­¦ä¹ ç®—æ³• | `LA-LORA` | ä½¿ç”¨ LA-LORAï¼ˆLayer-wise Adaptive LoRAï¼‰æ–¹æ³• |
| `--lr` | å®¢æˆ·ç«¯å­¦ä¹ ç‡ | `0.0001` | æ§åˆ¶æœ¬åœ°æ¨¡å‹æ›´æ–°çš„æ­¥é•¿ |
| `--data_name` | æ•°æ®é›†åç§° | `CIFAR100` | ä½¿ç”¨ CIFAR-100 å›¾åƒåˆ†ç±»æ•°æ®é›† |
| `--alpha_value` | Dirichlet åˆ†å¸ƒå‚æ•° | `0.1` | æ§åˆ¶æ•°æ®éç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆnon-IIDï¼‰ç¨‹åº¦ï¼Œè¶Šå°è¶Šä¸å‡åŒ€ |
| `--epoch` | æ€»è®­ç»ƒè½®æ•° | `101` | å…¨å±€è®­ç»ƒè½®æ•°ï¼ˆæ¯è½®é€‰éƒ¨åˆ†å®¢æˆ·ç«¯ï¼‰ |
| `--extname` | å®éªŒå¤‡æ³¨ | `CIFAR100` | ç”¨äºæ—¥å¿—æ–‡ä»¶åã€æ¨¡å‹ä¿å­˜åç­‰æ ‡è¯† |
| `--lr_decay` | å­¦ä¹ ç‡è¡°å‡ | `0.99` | æ¯è½®åå­¦ä¹ ç‡ä¹˜ä»¥è¯¥ç³»æ•° |
| `--CNN` | æ¨¡å‹æ¶æ„ | `swin_base` | ä½¿ç”¨ Swin-Transformer Base æ¨¡å‹ |
| `--E` | æœ¬åœ°è®­ç»ƒè½®æ•° | `1` | æ¯ä¸ªå®¢æˆ·ç«¯åœ¨æœ¬åœ°è®­ç»ƒçš„ epoch æ•° |
| `--batch_size` | æœ¬åœ° batch å¤§å° | `16` | æ¯ä¸ªå®¢æˆ·ç«¯æ¯æ¬¡è®­ç»ƒçš„æ ·æœ¬æ•° |
| `--gpu` | ä½¿ç”¨çš„ GPU ç¼–å· | `2` | æŒ‡å®šä½¿ç”¨ç¬¬ 2 å¼  GPU |
| `--p` | å¹¶è¡Œç»„æ•° | `1` | æ¯è½®å®¢æˆ·ç«¯åˆ†å‡ ç»„å¹¶è¡Œè¿è¡Œï¼ˆ1 è¡¨ç¤ºä¸åˆ†ç»„ï¼‰ |
| `--num_gpus_per` | æ¯ä¸ª Ray worker å ç”¨çš„ GPU æ¯”ä¾‹ | `0.25` | ä¸€å¼  GPU å¯åˆ†é…ç»™ 4 ä¸ª worker |
| `--selection` | æ¯è½®å®¢æˆ·ç«¯å‚ä¸æ¯”ä¾‹ | `0.5` | æ¯è½®ä» 8 ä¸ªå®¢æˆ·ç«¯ä¸­é€‰ 4 ä¸ªå‚ä¸è®­ç»ƒ |
| `--rho` | SAM ä¼˜åŒ–å™¨çš„æ‰°åŠ¨åŠå¾„ | `0.1` | ç”¨äºå¢å¼ºæ¨¡å‹æ³›åŒ–æ€§ |
| `--num_workers` | æ€»å®¢æˆ·ç«¯æ•° | `8` | æ¨¡æ‹Ÿ 8 ä¸ªå®¢æˆ·ç«¯ |
| `--pre` | æ˜¯å¦åŠ è½½é¢„è®­ç»ƒæƒé‡ | `1` | ä½¿ç”¨ ImageNet é¢„è®­ç»ƒçš„ Swin-Base |
| `--preprint` | æ¯å¤šå°‘è½®æ‰“å°ä¸€æ¬¡æ—¥å¿— | `5` | æ¯ 5 è½®è¾“å‡ºä¸€æ¬¡æµ‹è¯•ç²¾åº¦ |
| `--lora` | æ˜¯å¦å¯ç”¨ LoRA | `1` | å¯ç”¨ä½ç§©é€‚é…ï¼Œå‡å°‘å‚æ•°é‡ |
| `--r` | LoRA çš„ç§© | `16` | æ§åˆ¶ä½ç§©çŸ©é˜µçš„ç»´åº¦ |
| `--freeze` | æ˜¯å¦å†»ç»“é LoRA å±‚ | `0` | ä¸å†»ç»“ï¼Œæ‰€æœ‰å‚æ•°å‚ä¸è®­ç»ƒ |
| `--K` | æœ¬åœ°æœ€å¤§æ­¥æ•° | `50` | æ¯ä¸ªå®¢æˆ·ç«¯æ¯è½®æœ€å¤šè®­ç»ƒ 50 æ­¥ |
| `--optimizer` | ä¼˜åŒ–å™¨ | `AdamW` | ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨ï¼Œé€‚åˆ Transformer ç±»æ¨¡å‹ |

---

## âœ… ä¸€é”®è¿è¡Œè„šæœ¬

ä½ å¯ä»¥æŠŠä¸‹é¢ä¿å­˜ä¸º `run_cifar100.sh`ï¼ˆLinux/macOSï¼‰æˆ– `run_cifar100.bat`ï¼ˆWindowsï¼‰ï¼š

```bash
#!/bin/bash
python main_lora.py \
  --alg LA-LORA \
  --lr 0.0001 \
  --data_name CIFAR100 \
  --alpha_value 0.1 \
  --epoch 101 \
  --extname CIFAR100 \
  --lr_decay 0.99 \
  --CNN swin_base \
  --E 1 \
  --batch_size 16 \
  --gpu 2 \
  --p 1 \
  --num_gpus_per 0.25 \
  --selection 0.5 \
  --rho 0.1 \
  --num_workers 8 \
  --pre 1 \
  --preprint 5 \
  --lora 1 \
  --r 16 \
  --freeze 0 \
  --K 50 \
  --optimizer AdamW
```

èµ‹äºˆæ‰§è¡Œæƒé™ï¼ˆLinux/macOSï¼‰ï¼š
```bash
chmod +x run_cifar100.sh
./run_cifar100.sh
```

---

## ğŸ“ è¾“å‡ºæ–‡ä»¶
- **æ—¥å¿—æ–‡ä»¶**ï¼š`./log/LA-LORA-CIFAR100-...txt`
- **æ¨¡å‹ä¿å­˜**ï¼š`./model/...pth`
- **è®­ç»ƒæ›²çº¿**ï¼š`./plot/...npy`ï¼ˆåŒ…å« epoch, accuracy, lossï¼‰

---

## ğŸ“š ä¾èµ–å®‰è£…
```bash
pip install -r requirements.txt
```

---

å¦‚éœ€è¿è¡Œ NLP ä»»åŠ¡ï¼ˆå¦‚ BERTï¼‰ï¼Œè¯·ä½¿ç”¨ `main_LLM.py` å¹¶å‚è€ƒå¯¹åº”å‚æ•°ã€‚  
æœ‰é—®é¢˜æ¬¢è¿æ Issue æˆ– PRï¼


# ğŸ“– DP_LLM ä½¿ç”¨æŒ‡å—ï¼ˆMNLI + RoBERTa-Base + FedLORAï¼‰

æœ¬å‘½ä»¤ä½¿ç”¨ **RoBERTa-Base** æ¨¡å‹åœ¨ **MNLI** æ•°æ®é›†ä¸Šè¿è¡Œ **FedLORA** è”é‚¦å­¦ä¹ ç®—æ³•ï¼Œå¹¶å¯ç”¨ **å·®åˆ†éšç§ï¼ˆDPï¼‰**ã€‚  
ä»¥ä¸‹æ˜¯ä½ æä¾›çš„è¿è¡Œå‘½ä»¤çš„å®Œæ•´è§£æä¸è¯´æ˜ã€‚

---

## ğŸƒ è¿è¡Œå‘½ä»¤

```bash
python DP_LLM.py \
  --alg FedLORA \
  --lr 0.0002 \
  --data_name MNLI \
  --alpha_value 1 \
  --alpha 0.9 \
  --epoch 101 \
  --extname RTE \
  --lr_decay 1 \
  --gamma 0.3 \
  --CNN roberta_base \
  --E 1 \
  --batch_size 16 \
  --gpu 0 \
  --p 1 \
  --num_gpus_per 0.25 \
  --selection 0.2 \
  --rho 0.1 \
  --num_workers 20 \
  --pre 1 \
  --preprint 5 \
  --lora 1 \
  --r 8 \
  --freeze 1 \
  --K 20 \
  --optimizer AdamW
```

---

## ğŸ“– å‚æ•°è¯¦è§£

| å‚æ•° | å«ä¹‰ | ç¤ºä¾‹å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--alg` | è”é‚¦å­¦ä¹ ç®—æ³• | `FedLORA` | ä½¿ç”¨ FedLORAï¼ˆLoRA + FedAvgï¼‰ |
| `--lr` | å®¢æˆ·ç«¯å­¦ä¹ ç‡ | `0.0002` | æ§åˆ¶æœ¬åœ°æ¨¡å‹æ›´æ–°çš„æ­¥é•¿ |
| `--data_name` | æ•°æ®é›†åç§° | `MNLI` | ä½¿ç”¨ MNLIï¼ˆè‡ªç„¶è¯­è¨€æ¨ç†ä»»åŠ¡ï¼Œ3 åˆ†ç±»ï¼‰ |
| `--alpha_value` | Dirichlet åˆ†å¸ƒå‚æ•° | `1` | æ•°æ®åˆ†å¸ƒä¸º IIDï¼ˆå‡åŒ€åˆ†å¸ƒï¼‰ |
| `--alpha` | åŠ¨é‡ç³»æ•° | `0.9` | ç”¨äºæ›´æ–°æœåŠ¡å™¨ç«¯åŠ¨é‡ |
| `--epoch` | æ€»è®­ç»ƒè½®æ•° | `101` | å…¨å±€è®­ç»ƒè½®æ•° |
| `--extname` | å®éªŒå¤‡æ³¨ | `RTE` | ç”¨äºæ—¥å¿—æ–‡ä»¶åæ ‡è¯† |
| `--lr_decay` | å­¦ä¹ ç‡è¡°å‡ | `1` | ä¸è¡°å‡ï¼ˆä¿æŒæ’å®šï¼‰ |
| `--gamma` | åŠ¨é‡è¡°å‡ç³»æ•° | `0.3` | æ§åˆ¶æœåŠ¡å™¨åŠ¨é‡æ›´æ–°é€Ÿåº¦ |
| `--CNN` | æ¨¡å‹æ¶æ„ | `roberta_base` | ä½¿ç”¨ RoBERTa-Base æ¨¡å‹ |
| `--E` | æœ¬åœ°è®­ç»ƒè½®æ•° | `1` | æ¯ä¸ªå®¢æˆ·ç«¯æœ¬åœ°è®­ç»ƒ 1 ä¸ª epoch |
| `--batch_size` | æœ¬åœ° batch å¤§å° | `16` | æ¯ä¸ªå®¢æˆ·ç«¯æ¯æ¬¡è®­ç»ƒçš„æ ·æœ¬æ•° |
| `--gpu` | ä½¿ç”¨çš„ GPU ç¼–å· | `0` | æŒ‡å®šä½¿ç”¨ç¬¬ 0 å¼  GPU |
| `--p` | å¹¶è¡Œç»„æ•° | `1` | æ¯è½®å®¢æˆ·ç«¯ä¸åˆ†ç»„å¹¶è¡Œ |
| `--num_gpus_per` | æ¯ä¸ª Ray worker å ç”¨ GPU æ¯”ä¾‹ | `0.25` | ä¸€å¼  GPU å¯åˆ†é…ç»™ 4 ä¸ª worker |
| `--selection` | æ¯è½®å®¢æˆ·ç«¯å‚ä¸æ¯”ä¾‹ | `0.2` | æ¯è½®ä» 20 ä¸ªå®¢æˆ·ç«¯ä¸­é€‰ 4 ä¸ªå‚ä¸è®­ç»ƒ |
| `--rho` | SAM ä¼˜åŒ–å™¨æ‰°åŠ¨åŠå¾„ | `0.1` | å¢å¼ºæ¨¡å‹é²æ£’æ€§ |
| `--num_workers` | æ€»å®¢æˆ·ç«¯æ•° | `20` | æ¨¡æ‹Ÿ 20 ä¸ªå®¢æˆ·ç«¯ |
| `--pre` | æ˜¯å¦åŠ è½½é¢„è®­ç»ƒæƒé‡ | `1` | ä½¿ç”¨ HuggingFace é¢„è®­ç»ƒçš„ RoBERTa-Base |
| `--preprint` | æ¯å¤šå°‘è½®æ‰“å°ä¸€æ¬¡æ—¥å¿— | `5` | æ¯ 5 è½®è¾“å‡ºä¸€æ¬¡æµ‹è¯•ç²¾åº¦ |
| `--lora` | æ˜¯å¦å¯ç”¨ LoRA | `1` | å¯ç”¨ä½ç§©é€‚é…ï¼Œå‡å°‘å‚æ•°é‡ |
| `--r` | LoRA çš„ç§© | `8` | æ§åˆ¶ä½ç§©çŸ©é˜µç»´åº¦ï¼ˆè¶Šå°è¶Šè½»é‡ï¼‰ |
| `--freeze` | æ˜¯å¦å†»ç»“é LoRA å±‚ | `1` | å†»ç»“é LoRA å±‚ï¼Œä»…è®­ç»ƒ LoRA å‚æ•° |
| `--K` | æœ¬åœ°æœ€å¤§æ­¥æ•° | `20` | æ¯è½®æœ€å¤šè®­ç»ƒ 20 æ­¥ |
| `--optimizer` | ä¼˜åŒ–å™¨ | `AdamW` | ä½¿ç”¨ AdamWï¼Œé€‚åˆ Transformer |

---

## ğŸ” å·®åˆ†éšç§ï¼ˆDPï¼‰ç›¸å…³å‚æ•°ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--privacy` | `1` | å¯ç”¨å·®åˆ†éšç§ |
| `--dp_sigma` | `0.1` | å™ªå£°ä¹˜å­ï¼ˆè¶Šå¤§éšç§è¶Šå¼ºï¼Œç²¾åº¦è¶Šä½ï¼‰ |
| `--C` | `1.0` | æ¢¯åº¦è£å‰ªèŒƒæ•° |

---

## âœ… ä¸€é”®è¿è¡Œè„šæœ¬

ä¿å­˜ä¸º `run_mnli_dp.sh`ï¼ˆLinux/macOSï¼‰ï¼š

```bash
#!/bin/bash
python DP_LLM.py \
  --alg FedLORA \
  --lr 0.0002 \
  --data_name MNLI \
  --alpha_value 1 \
  --alpha 0.9 \
  --epoch 101 \
  --extname RTE \
  --lr_decay 1 \
  --gamma 0.3 \
  --CNN roberta_base \
  --E 1 \
  --batch_size 16 \
  --gpu 0 \
  --p 1 \
  --num_gpus_per 0.25 \
  --selection 0.2 \
  --rho 0.1 \
  --num_workers 20 \
  --pre 1 \
  --preprint 5 \
  --lora 1 \
  --r 8 \
  --freeze 1 \
  --K 20 \
  --optimizer AdamW
```

èµ‹äºˆæ‰§è¡Œæƒé™ï¼š
```bash
chmod +x run_mnli_dp.sh
./run_mnli_dp.sh
```

---

## ğŸ“ è¾“å‡ºæ–‡ä»¶
- **æ—¥å¿—æ–‡ä»¶**ï¼š`./log/FedLORA-MNLI-...txt`
- **æ¨¡å‹ä¿å­˜**ï¼š`./model/...pth`
- **è®­ç»ƒæ›²çº¿**ï¼š`./plot/...npy`

---

## ğŸ“¦ ä¾èµ–å®‰è£…
```bash
pip install -r requirements.txt
```

---

å¦‚éœ€è¿è¡Œé DP ç‰ˆæœ¬ï¼Œè¯·æ”¹ç”¨ `main_LLM.py`ã€‚  
æœ‰é—®é¢˜æ¬¢è¿æ Issue æˆ– PRï¼
---

## ğŸ“Š Benchmarks

| **Dataset** | **Model** | **Algorithm** | **Accuracy** | **Speedup (LoRA)** |
|-------------|-----------|---------------|--------------|---------------------|
| **SST-2**   | BERT-base | FedLORA       | **94.2%**    | **120Ã— faster**     |
| **CIFAR-100** | ViT-B    | FedLORA       | **87.5%**    | **95Ã— less comms**  |
| **Tiny-ImageNet** | Swin-Tiny | FedLORA  | **72.1%**    | **100Ã— fewer params** |

---

## ğŸ“ Code Structure

```
FedLoRA/
â”œâ”€â”€ main_LLM.py          # NLP experiments (BERT, RoBERTa)
â”œâ”€â”€ main_lora.py         # Vision experiments (ViT, Swin, ResNet)
â”œâ”€â”€ dirichlet_data.py    # Non-IID data partitioning
â”œâ”€â”€ sam.py               # SAM optimizer implementation
â”œâ”€â”€ requirements.txt     # Dependencies
â”œâ”€â”€ README.md            # This file
â””â”€â”€ logs/                # Training logs
```

---

## ğŸ› ï¸ Advanced Usage

### **Custom LoRA Config**
```python
from peft import LoraConfig
lora_config = LoraConfig(
    r=16,               # Rank
    lora_alpha=32,      # Scaling
    target_modules=["query", "value"],  # Apply to attention layers
    lora_dropout=0.1,
    bias="none",
)
```

### **DP-SGD with Privacy Budget**
```bash
python main_lora.py \
  --privacy 1 \
  --dp_sigma 0.1 \
  --C 1.0  # Gradient clipping norm
```

### **Multi-GPU Training**
```bash
python main_lora.py \
  --gpu 0,1,2,3 \
  --num_gpus_per 0.25  # 4 GPUs, 0.25 per worker
```

---

## ğŸ“ˆ Visualization

- **TensorBoard logs** are saved in `logs/`.
- Run:
  ```bash
  tensorboard --logdir logs/
  ```

---

## ğŸ¤ Contributing

We welcome contributions! Please:

1. Fork the repo.
2. Create a feature branch (`git checkout -b feature/new-algo`).
3. Submit a PR with clear descriptions.

---

## ğŸ“œ License

This project is licensed under the **MIT License** - see [LICENSE](LICENSE) for details.

---


---

## ğŸ™‹ FAQ

**Q: Can I use this for non-LoRA methods?**
A: Yes! Set `--lora 0` to disable LoRA and use full fine-tuning.

**Q: How to add a new dataset?**
A: Modify `get_data_loader()` in `main_LLM.py` or `main_lora.py`.

**Q: Does it support heterogeneous clients?**
A: Yes! Use `--alpha_value 0.1` for high data heterogeneity.

---

â­ **Star** this repo if you find it useful!